{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "** Claude COde** italicized text"
      ],
      "metadata": {
        "id": "Tsoxn-EP2rg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGaVKM1A2dm6",
        "outputId": "03d0ac6e-2a92-4f3f-c29e-82d5536bdcf3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=e9c925410af9976cb19ac52bcdc9c5387a8127b8060c611839056b5bafe5ec50\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Google Colab Drive Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define results directory\n",
        "import os\n",
        "results_dir = '/content/drive/MyDrive/Amir-Khan FYP/Cluade-Results'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Create visualization subdirectory\n",
        "viz_dir = os.path.join(results_dir, 'Visualizations')\n",
        "os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "# 2) Memory-Optimized Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, learning_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
        "                            roc_curve, roc_auc_score, auc, precision_recall_curve, average_precision_score,\n",
        "                            f1_score, accuracy_score, recall_score, precision_score)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "import gc\n",
        "import psutil\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.colors as mcolors\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy import stats\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure memory-efficient settings\n",
        "plt.style.use('ggplot')\n",
        "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
        "plt.rcParams.update({\n",
        "    'figure.dpi': 300,\n",
        "    'savefig.dpi': 300,\n",
        "    'font.size': 10,\n",
        "    'axes.titlesize': 14,\n",
        "    'axes.titlepad': 15,\n",
        "    'axes.labelpad': 10\n",
        "})\n",
        "\n",
        "# Define consistent color palette for visualizations\n",
        "colors = sns.color_palette(\"viridis\", 10)\n",
        "model_colors = {\n",
        "    'LightGBM': colors[0],\n",
        "    'XGBoost': colors[1],\n",
        "    'RandomForest': colors[2]\n",
        "}\n",
        "\n",
        "# 3) Memory-Aware Data Loading\n",
        "def load_data(path):\n",
        "    \"\"\"Load data with memory optimization\"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Downcast numerical types\n",
        "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    df[num_cols] = df[num_cols].apply(pd.to_numeric, downcast='unsigned' if 'int' in str(df[num_cols].dtypes[0]) else 'float')\n",
        "\n",
        "    return df\n",
        "\n",
        "data_path = '/content/drive/MyDrive/Amir-Khan FYP/feature_vectors_syscallsbinders_frequency_5_Cat.csv'\n",
        "df = load_data(data_path)\n",
        "print(f\"Initial memory usage: {df.memory_usage().sum()/1024**2:.2f} MB\")\n",
        "\n",
        "# 4) Data Exploration and Initial Visualizations\n",
        "def plot_class_distribution(y, save_path):\n",
        "    \"\"\"Plot the class distribution with enhanced styling\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Count plot with custom styling\n",
        "    ax = sns.countplot(x=y, palette='viridis')\n",
        "\n",
        "    # Add count labels on top of bars\n",
        "    total = len(y)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        percentage = 100 * height / total\n",
        "        ax.text(p.get_x() + p.get_width()/2., height + 0.1,\n",
        "                f'{int(height)}\\n({percentage:.1f}%)',\n",
        "                ha=\"center\", fontsize=9)\n",
        "\n",
        "    plt.title('Class Distribution in Dataset', fontsize=16, pad=20)\n",
        "    plt.xlabel('Class', fontsize=12)\n",
        "    plt.ylabel('Count', fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_feature_correlations(X, save_path, n_features=20):\n",
        "    \"\"\"Plot correlation heatmap of top features\"\"\"\n",
        "    # Select top features by variance\n",
        "    X_sample = X.iloc[:, :n_features] if X.shape[1] > n_features else X\n",
        "\n",
        "    plt.figure(figsize=(14, 12))\n",
        "    corr = X_sample.corr()\n",
        "\n",
        "    # Generate mask for the upper triangle\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "    # Generate a custom diverging colormap\n",
        "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "    # Draw the heatmap with the mask and correct aspect ratio\n",
        "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin=-1.0, center=0,\n",
        "                square=True, linewidths=.5, annot=False, fmt=\".2f\",\n",
        "                cbar_kws={\"shrink\": .8, \"label\": \"Correlation Coefficient\"})\n",
        "\n",
        "    plt.title('Feature Correlation Matrix (Top Features)', fontsize=16, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_feature_importance(X, y, save_path, n_features=20):\n",
        "    \"\"\"Plot feature importance using SelectKBest\"\"\"\n",
        "    # Select k best features\n",
        "    k = min(n_features, X.shape[1])\n",
        "    selector = SelectKBest(f_classif, k=k)\n",
        "    selector.fit(X, y)\n",
        "\n",
        "    # Get scores and feature names\n",
        "    scores = -np.log10(selector.pvalues_)\n",
        "    feature_names = X.columns\n",
        "\n",
        "    if len(feature_names) > k:\n",
        "        # Sort and get top k features\n",
        "        indices = np.argsort(scores)[::-1][:k]\n",
        "        top_scores = scores[indices]\n",
        "        top_features = [feature_names[i] for i in indices]\n",
        "    else:\n",
        "        # Use all features if less than k\n",
        "        indices = np.argsort(scores)[::-1]\n",
        "        top_scores = scores[indices]\n",
        "        top_features = [feature_names[i] for i in indices]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(range(len(top_features)), top_scores, align='center', color=colors[3:])\n",
        "    plt.yticks(range(len(top_features)), top_features)\n",
        "    plt.title('Top Feature Importance (ANOVA F-value)', fontsize=16, pad=20)\n",
        "    plt.xlabel('-log10(p-value)', fontsize=12)\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Execute initial visualizations\n",
        "plot_class_distribution(df['Class'], os.path.join(viz_dir, 'class_distribution.png'))\n",
        "plot_feature_correlations(df.drop('Class', axis=1), os.path.join(viz_dir, 'feature_correlation.png'))\n",
        "plot_feature_importance(df.drop('Class', axis=1), df['Class'], os.path.join(viz_dir, 'feature_importance.png'))\n",
        "\n",
        "# 5) Balanced Sampling Strategy\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Initial split with smaller test size\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.10,  # Reduced from 0.15\n",
        "    random_state=56,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# Plot train-test split visualization\n",
        "def plot_train_test_split(y_train, y_test, save_path):\n",
        "    \"\"\"Visualize the class distribution in train and test sets with corrected percentages\"\"\"\n",
        "    # Count occurrences of each class\n",
        "    train_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "    test_counts = pd.Series(y_test).value_counts().sort_index()\n",
        "\n",
        "    # Create a DataFrame for plotting counts\n",
        "    df_plot = pd.DataFrame({\n",
        "        'Train': train_counts,\n",
        "        'Test': test_counts\n",
        "    })\n",
        "\n",
        "    # Calculate correct percentages within each set\n",
        "    total_train = len(y_train)\n",
        "    total_test = len(y_test)\n",
        "\n",
        "    train_percentages = (train_counts / total_train) * 100\n",
        "    test_percentages = (test_counts / total_test) * 100\n",
        "\n",
        "    # Create DataFrame for percentages\n",
        "    df_percentages = pd.DataFrame({\n",
        "        'Train %': train_percentages,\n",
        "        'Test %': test_percentages\n",
        "    })\n",
        "\n",
        "    # Print actual values for verification\n",
        "    print(\"Train counts:\", dict(train_counts))\n",
        "    print(\"Test counts:\", dict(test_counts))\n",
        "    print(\"Train percentages:\", dict(train_percentages))\n",
        "    print(\"Test percentages:\", dict(test_percentages))\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    # Plot counts\n",
        "    df_plot.plot(kind='bar', ax=ax1, width=0.8, color=[colors[0], colors[1]])\n",
        "    ax1.set_title('Class Distribution (Counts)', fontsize=14)\n",
        "    ax1.set_xlabel('Class', fontsize=12)\n",
        "    ax1.set_ylabel('Count', fontsize=12)\n",
        "    ax1.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add count labels\n",
        "    for container in ax1.containers:\n",
        "        ax1.bar_label(container, fontsize=9)\n",
        "\n",
        "    # Plot percentages\n",
        "    df_percentages.plot(kind='bar', ax=ax2, width=0.8, color=[colors[0], colors[1]])\n",
        "    ax2.set_title('Class Distribution (Percentage)', fontsize=14)\n",
        "    ax2.set_xlabel('Class', fontsize=12)\n",
        "    ax2.set_ylabel('Percentage (%)', fontsize=12)\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add percentage labels\n",
        "    for container in ax2.containers:\n",
        "        ax2.bar_label(container, fmt='%.1f%%', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "plot_train_test_split(y_train, y_test, os.path.join(viz_dir, 'train_test_split.png'))\n",
        "\n",
        "# Clean up memory\n",
        "del df\n",
        "gc.collect()\n",
        "\n",
        "# Apply SMOTE with sampling strategy\n",
        "smote = SMOTE(\n",
        "    sampling_strategy='auto',  # Auto balances to largest class\n",
        "    random_state=42,\n",
        "    k_neighbors=3  # Reduced from default 5\n",
        ")\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Visualize SMOTE effect\n",
        "def plot_smote_effect(y_orig, y_resampled, save_path):\n",
        "    \"\"\"Visualize the effect of SMOTE on class distribution\"\"\"\n",
        "    # Count occurrences of each class\n",
        "    orig_counts = pd.Series(y_orig).value_counts().sort_index()\n",
        "    resamp_counts = pd.Series(y_resampled).value_counts().sort_index()\n",
        "\n",
        "    # Create a DataFrame for plotting\n",
        "    df_plot = pd.DataFrame({\n",
        "        'Original': orig_counts,\n",
        "        'After SMOTE': resamp_counts\n",
        "    })\n",
        "\n",
        "    # Calculate class ratios\n",
        "    total_orig = len(y_orig)\n",
        "    total_resamp = len(y_resampled)\n",
        "\n",
        "    df_percentages = pd.DataFrame({\n",
        "        'Original %': (orig_counts / total_orig) * 100,\n",
        "        'After SMOTE %': (resamp_counts / total_resamp) * 100\n",
        "    })\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "    # Plot counts\n",
        "    df_plot.plot(kind='bar', ax=ax1, width=0.8, color=[colors[3], colors[4]])\n",
        "    ax1.set_title('Effect of SMOTE (Counts)', fontsize=14)\n",
        "    ax1.set_xlabel('Class', fontsize=12)\n",
        "    ax1.set_ylabel('Count', fontsize=12)\n",
        "    ax1.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add count labels\n",
        "    for container in ax1.containers:\n",
        "        ax1.bar_label(container, fontsize=9)\n",
        "\n",
        "    # Plot percentages\n",
        "    df_percentages.plot(kind='bar', ax=ax2, width=0.8, color=[colors[3], colors[4]])\n",
        "    ax2.set_title('Effect of SMOTE (Percentage)', fontsize=14)\n",
        "    ax2.set_xlabel('Class', fontsize=12)\n",
        "    ax2.set_ylabel('Percentage (%)', fontsize=12)\n",
        "    ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add percentage labels\n",
        "    for container in ax2.containers:\n",
        "        ax2.bar_label(container, fmt='%.1f%%', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "plot_smote_effect(y_train, y_train_res, os.path.join(viz_dir, 'smote_effect.png'))\n",
        "\n",
        "# Clean up original training data\n",
        "del X_train, y_train\n",
        "gc.collect()\n",
        "\n",
        "# 6) Memory-Efficient Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_res)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 7) Feature Transformation and Dimensionality Reduction\n",
        "def plot_pca_explained_variance(X_scaled, save_path, n_components=20):\n",
        "    \"\"\"Plot PCA explained variance ratio with error handling for small feature dimensions\"\"\"\n",
        "    # Get the actual number of features\n",
        "    n_features = X_scaled.shape[1]\n",
        "\n",
        "    # Print warning if very few features\n",
        "    if n_features < 5:\n",
        "        print(f\"Warning: Input data has only {n_features} features. PCA analysis may be limited.\")\n",
        "\n",
        "    # Limit components to feature dimension\n",
        "    n_components = min(n_components, n_features)\n",
        "\n",
        "    # Fit PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca.fit(X_scaled)\n",
        "\n",
        "    # Get explained variance\n",
        "    exp_var_ratio = pca.explained_variance_ratio_\n",
        "    cum_exp_var_ratio = np.cumsum(exp_var_ratio)\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Plot explained variance ratio\n",
        "    ax1.bar(range(1, len(exp_var_ratio) + 1), exp_var_ratio,\n",
        "           alpha=0.7, color=colors[:len(exp_var_ratio)], edgecolor='black', linewidth=0.5)\n",
        "    ax1.set_title('Explained Variance Ratio by Component', fontsize=14)\n",
        "    ax1.set_xlabel('Principal Component', fontsize=12)\n",
        "    ax1.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
        "\n",
        "    # Adjust x-ticks based on number of components\n",
        "    if len(exp_var_ratio) > 10:\n",
        "        ax1.set_xticks(range(1, len(exp_var_ratio) + 1, 2))\n",
        "    else:\n",
        "        ax1.set_xticks(range(1, len(exp_var_ratio) + 1))\n",
        "\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Plot cumulative explained variance\n",
        "    ax2.plot(range(1, len(cum_exp_var_ratio) + 1), cum_exp_var_ratio,\n",
        "            marker='o', linestyle='-', color=colors[5], linewidth=2, markersize=6)\n",
        "    ax2.set_title('Cumulative Explained Variance', fontsize=14)\n",
        "    ax2.set_xlabel('Number of Components', fontsize=12)\n",
        "    ax2.set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
        "    ax2.grid(alpha=0.3)\n",
        "\n",
        "    # Adjust x-ticks based on number of components\n",
        "    if len(cum_exp_var_ratio) > 10:\n",
        "        ax2.set_xticks(range(1, len(cum_exp_var_ratio) + 1, 2))\n",
        "    else:\n",
        "        ax2.set_xticks(range(1, len(cum_exp_var_ratio) + 1))\n",
        "\n",
        "    # Add threshold line at 0.95\n",
        "    ax2.axhline(y=0.95, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "    # Find number of components needed for 95% variance\n",
        "    if any(cum_exp_var_ratio >= 0.95):\n",
        "        n_components_95 = np.argmax(cum_exp_var_ratio >= 0.95) + 1\n",
        "        ax2.text(min(n_components_95 + 0.5, len(cum_exp_var_ratio)), 0.96,\n",
        "                f'{n_components_95} components for 95% variance',\n",
        "                fontsize=10, color='red')\n",
        "    else:\n",
        "        # In case we don't reach 95% explained variance\n",
        "        n_components_95 = len(cum_exp_var_ratio)\n",
        "        ax2.text(len(cum_exp_var_ratio) * 0.7, 0.96,\n",
        "                f'All {n_components_95} components explain {cum_exp_var_ratio[-1]:.2%} variance',\n",
        "                fontsize=10, color='red')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Return number of components for 95% variance\n",
        "    return n_components_95\n",
        "\n",
        "# Apply PCA and plot explained variance\n",
        "n_components_95 = plot_pca_explained_variance(X_train_scaled,\n",
        "                                             os.path.join(viz_dir, 'pca_explained_variance.png'))\n",
        "print(f\"Number of components needed for 95% variance: {n_components_95}\")\n",
        "\n",
        "# Apply PCA for visualization and dimensionality reduction\n",
        "# Ensure we have at least 3 components for visualization if possible\n",
        "n_components_viz = max(3, min(n_components_95, X_train_scaled.shape[1]))\n",
        "\n",
        "# If feature dimension is less than 3, use what we have\n",
        "if X_train_scaled.shape[1] < 3:\n",
        "    n_components_viz = X_train_scaled.shape[1]\n",
        "    print(f\"Warning: Input data has only {X_train_scaled.shape[1]} features. Will use all available features for PCA.\")\n",
        "\n",
        "pca = PCA(n_components=n_components_viz)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "print(f\"PCA transformation complete. Output shape: {X_train_pca.shape}\")\n",
        "\n",
        "# 8) Enhanced Visualizations\n",
        "def plot_tsne(X, y, save_path, subsample=2000, perplexity=30, title=\"t-SNE Visualization\"):\n",
        "    \"\"\"Memory-efficient t-SNE visualization\"\"\"\n",
        "    if len(X) > subsample:\n",
        "        idx = np.random.choice(len(X), subsample, replace=False)\n",
        "        X_sub = X[idx]\n",
        "        y_sub = y.iloc[idx] if isinstance(y, pd.Series) else y[idx]\n",
        "    else:\n",
        "        X_sub = X\n",
        "        y_sub = y\n",
        "\n",
        "    # Apply t-SNE\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "    X_tsne = tsne.fit_transform(X_sub)\n",
        "\n",
        "    # Create a DataFrame for easier plotting\n",
        "    df_tsne = pd.DataFrame({\n",
        "        'x': X_tsne[:, 0],\n",
        "        'y': X_tsne[:, 1],\n",
        "        'class': y_sub\n",
        "    })\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Create a scatter plot with custom styling\n",
        "    scatter = sns.scatterplot(\n",
        "        x='x', y='y',\n",
        "        hue='class',\n",
        "        data=df_tsne,\n",
        "        palette='viridis',\n",
        "        alpha=0.8,\n",
        "        s=60,\n",
        "        edgecolor='w',\n",
        "        linewidth=0.5\n",
        "    )\n",
        "\n",
        "    # Improve legend\n",
        "    plt.legend(title='Class', title_fontsize=12, fontsize=10,\n",
        "              loc='best', frameon=True, framealpha=0.7)\n",
        "\n",
        "    # Remove axis ticks and labels\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xlabel('')\n",
        "    plt.ylabel('')\n",
        "\n",
        "    # Add title and borders\n",
        "    plt.title(title, fontsize=16, pad=20)\n",
        "    plt.grid(False)\n",
        "\n",
        "    # Add contours around clusters\n",
        "    for cls in np.unique(y_sub):\n",
        "        df_class = df_tsne[df_tsne['class'] == cls]\n",
        "        if len(df_class) > 10:  # Only draw contour if there are enough points\n",
        "            try:\n",
        "                # Calculate mean and std of this class\n",
        "                mean_x, mean_y = df_class['x'].mean(), df_class['y'].mean()\n",
        "                std_x, std_y = df_class['x'].std(), df_class['y'].std()\n",
        "\n",
        "                # Add class label at the center of each cluster\n",
        "                plt.text(mean_x, mean_y, str(cls),\n",
        "                        fontsize=12, ha='center', va='center',\n",
        "                        bbox=dict(boxstyle=\"round,pad=0.3\",\n",
        "                                 fc='white', ec=\"gray\", alpha=0.8))\n",
        "            except:\n",
        "                pass  # Skip if there's an error in contour calculation\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def plot_pca_visualization(X_pca, y, save_path, three_dim=True):\n",
        "    \"\"\"Create 2D and 3D PCA visualizations with error handling for low-dimensional data\"\"\"\n",
        "    # Check if we have enough dimensions for even 2D visualization\n",
        "    if X_pca.shape[1] < 2:\n",
        "        print(f\"Warning: PCA output has only {X_pca.shape[1]} dimension(s). Creating 1D visualization instead.\")\n",
        "\n",
        "        # Create a 1D visualization\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Create a DataFrame with PC1 and a dummy y-axis\n",
        "        df_pca = pd.DataFrame({\n",
        "            'PC1': X_pca[:, 0],\n",
        "            'class': y\n",
        "        })\n",
        "\n",
        "        # Add jitter for better visualization in 1D\n",
        "        jitter = np.random.normal(0, 0.05, size=len(df_pca))\n",
        "\n",
        "        # Create scatter plot with PC1 and jitter\n",
        "        scatter = sns.scatterplot(\n",
        "            x='PC1', y=jitter,\n",
        "            hue='class',\n",
        "            data=df_pca,\n",
        "            palette='viridis',\n",
        "            alpha=0.8,\n",
        "            s=60,\n",
        "            edgecolor='w',\n",
        "            linewidth=0.5\n",
        "        )\n",
        "\n",
        "        plt.title('PCA Visualization (1D with jitter)', fontsize=16, pad=20)\n",
        "        plt.xlabel('Principal Component 1', fontsize=12)\n",
        "        plt.ylabel('Jitter (for visualization only)', fontsize=12)\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.legend(title='Class', title_fontsize=12, fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        return\n",
        "\n",
        "    # Create a DataFrame for easier plotting\n",
        "    df_pca = pd.DataFrame({\n",
        "        'PC1': X_pca[:, 0],\n",
        "        'PC2': X_pca[:, 1],\n",
        "        'class': y\n",
        "    })\n",
        "\n",
        "    if three_dim and X_pca.shape[1] >= 3:\n",
        "        df_pca['PC3'] = X_pca[:, 2]\n",
        "\n",
        "        # Create figure with two subplots (2D and 3D)\n",
        "        fig = plt.figure(figsize=(18, 9))\n",
        "\n",
        "        # 2D Plot\n",
        "        ax1 = fig.add_subplot(121)\n",
        "        scatter_2d = sns.scatterplot(\n",
        "            x='PC1', y='PC2',\n",
        "            hue='class',\n",
        "            data=df_pca,\n",
        "            palette='viridis',\n",
        "            alpha=0.8,\n",
        "            s=60,\n",
        "            edgecolor='w',\n",
        "            linewidth=0.5,\n",
        "            ax=ax1\n",
        "        )\n",
        "        ax1.set_title('PCA Visualization (2D: PC1 vs PC2)', fontsize=14)\n",
        "        ax1.set_xlabel('Principal Component 1', fontsize=12)\n",
        "        ax1.set_ylabel('Principal Component 2', fontsize=12)\n",
        "        ax1.grid(alpha=0.3)\n",
        "\n",
        "        # 3D Plot\n",
        "        ax2 = fig.add_subplot(122, projection='3d')\n",
        "        classes = np.unique(y)\n",
        "        colors_dict = dict(zip(classes, sns.color_palette(\"viridis\", len(classes))))\n",
        "\n",
        "        for cls in classes:\n",
        "            subset = df_pca[df_pca['class'] == cls]\n",
        "            ax2.scatter(\n",
        "                subset['PC1'], subset['PC2'], subset['PC3'],\n",
        "                label=cls,\n",
        "                color=colors_dict[cls],\n",
        "                alpha=0.8,\n",
        "                s=60,\n",
        "                edgecolor='w',\n",
        "                linewidth=0.5\n",
        "            )\n",
        "\n",
        "        ax2.set_title('PCA Visualization (3D: PC1, PC2, PC3)', fontsize=14)\n",
        "        ax2.set_xlabel('Principal Component 1', fontsize=10)\n",
        "        ax2.set_ylabel('Principal Component 2', fontsize=10)\n",
        "        ax2.set_zlabel('Principal Component 3', fontsize=10)\n",
        "        ax2.legend(title='Class', title_fontsize=12, fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        # Only 2D Plot\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        scatter = sns.scatterplot(\n",
        "            x='PC1', y='PC2',\n",
        "            hue='class',\n",
        "            data=df_pca,\n",
        "            palette='viridis',\n",
        "            alpha=0.8,\n",
        "            s=60,\n",
        "            edgecolor='w',\n",
        "            linewidth=0.5\n",
        "        )\n",
        "        plt.title('PCA Visualization (PC1 vs PC2)', fontsize=16, pad=20)\n",
        "        plt.xlabel('Principal Component 1', fontsize=12)\n",
        "        plt.ylabel('Principal Component 2', fontsize=12)\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.legend(title='Class', title_fontsize=12, fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "# Plot t-SNE and PCA visualizations\n",
        "plot_tsne(X_train_scaled, y_train_res, os.path.join(viz_dir, 'tsne_training_data.png'),\n",
        "         title=\"t-SNE Visualization of Training Data (after SMOTE)\")\n",
        "plot_pca_visualization(X_train_pca, y_train_res, os.path.join(viz_dir, 'pca_visualization.png'))\n",
        "\n",
        "# 9) Memory-Optimized Model Training\n",
        "models = {\n",
        "    'LightGBM': LGBMClassifier(verbose=-1, device='cpu'),\n",
        "    'XGBoost': XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='mlogloss',\n",
        "        tree_method='hist'  # More memory-efficient\n",
        "    ),\n",
        "    'RandomForest': RandomForestClassifier(\n",
        "        max_depth=10,  # Restricted depth\n",
        "        n_jobs=-1\n",
        "    )\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'LightGBM': {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [3, 5],\n",
        "        'learning_rate': [0.05, 0.1]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [3, 5],\n",
        "        'learning_rate': [0.05, 0.1]\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'n_estimators': [50, 100],\n",
        "        'max_depth': [5, 10],\n",
        "        'min_samples_split': [5, 10]  # More conservative splits\n",
        "    }\n",
        "}\n",
        "\n",
        "# Memory monitoring function\n",
        "def check_memory():\n",
        "    return psutil.virtual_memory().percent\n",
        "\n",
        "# Learning curve function\n",
        "def plot_learning_curve(estimator, X, y, train_sizes, cv, save_path, title=\"Learning Curve\"):\n",
        "    \"\"\"Plot learning curve for an estimator\"\"\"\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, train_sizes=train_sizes, cv=cv,\n",
        "        scoring='f1_weighted', n_jobs=-1\n",
        "    )\n",
        "\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Plot mean training and test scores\n",
        "    plt.plot(train_sizes, train_mean, 'o-', color=colors[0], label=\"Training score\", linewidth=2)\n",
        "    plt.plot(train_sizes, test_mean, 'o-', color=colors[2], label=\"Cross-validation score\", linewidth=2)\n",
        "\n",
        "    # Plot standard deviation bands\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=colors[0])\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color=colors[2])\n",
        "\n",
        "    # Plot details\n",
        "    plt.title(title, fontsize=16, pad=20)\n",
        "    plt.xlabel(\"Training examples\", fontsize=12)\n",
        "    plt.ylabel(\"F1 Score (weighted)\", fontsize=12)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.legend(loc=\"best\", fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Confusion matrix plotting function\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, save_path, title=\"Confusion Matrix\"):\n",
        "    \"\"\"Plot a confusion matrix with improved styling and robust error handling\"\"\"\n",
        "    # Get the unique classes from the data itself\n",
        "    unique_true = np.unique(y_true)\n",
        "    unique_pred = np.unique(y_pred)\n",
        "\n",
        "    # Create confusion matrix directly from data\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Create a normalized version\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        cm_norm = np.nan_to_num(cm_norm)  # Replace NaNs with 0\n",
        "\n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "    # Plot raw counts - direct approach without DataFrame conversion\n",
        "    im1 = ax1.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "    ax1.set_title('Confusion Matrix (Counts)', fontsize=14)\n",
        "\n",
        "    # Create tick marks\n",
        "    tick_marks = np.arange(len(unique_true))\n",
        "    ax1.set_xticks(tick_marks)\n",
        "    ax1.set_yticks(tick_marks)\n",
        "\n",
        "    # Label ticks with class names\n",
        "    ax1.set_xticklabels(unique_true, rotation=45, ha='right')\n",
        "    ax1.set_yticklabels(unique_true)\n",
        "\n",
        "    # Add text annotations\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        ax1.text(j, i, format(cm[i, j], 'd'),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    # Add colorbar\n",
        "    plt.colorbar(im1, ax=ax1)\n",
        "\n",
        "    # Labels and ticks\n",
        "    ax1.set_ylabel('True Label', fontsize=12)\n",
        "    ax1.set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "    # Plot normalized percentages - direct approach\n",
        "    im2 = ax2.imshow(cm_norm, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n",
        "    ax2.set_title('Confusion Matrix (Normalized)', fontsize=14)\n",
        "\n",
        "    # Create tick marks\n",
        "    ax2.set_xticks(tick_marks)\n",
        "    ax2.set_yticks(tick_marks)\n",
        "\n",
        "    # Label ticks with class names\n",
        "    ax2.set_xticklabels(unique_true, rotation=45, ha='right')\n",
        "    ax2.set_yticklabels(unique_true)\n",
        "\n",
        "    # Add text annotations\n",
        "    for i, j in itertools.product(range(cm_norm.shape[0]), range(cm_norm.shape[1])):\n",
        "        ax2.text(j, i, format(cm_norm[i, j], '.2%'),\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if cm_norm[i, j] > 0.5 else \"black\")\n",
        "\n",
        "    # Add colorbar\n",
        "    plt.colorbar(im2, ax=ax2)\n",
        "\n",
        "    # Labels\n",
        "    ax2.set_ylabel('True Label', fontsize=12)\n",
        "    ax2.set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "    plt.suptitle(title, fontsize=16, y=1.05)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# ROC curve plotting function\n",
        "def plot_roc_curves(models_dict, X_test, y_test, classes, save_path):\n",
        "    \"\"\"Plot ROC curves for multiple models\"\"\"\n",
        "    n_classes = len(classes)\n",
        "    le = LabelEncoder().fit(classes)\n",
        "    y_test_enc = le.transform(y_test)\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # If binary classification\n",
        "    if n_classes == 2:\n",
        "        for model_name, model in models_dict.items():\n",
        "            # Get predictions\n",
        "            y_score = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            # Compute ROC curve and area\n",
        "            fpr, tpr, _ = roc_curve(y_test_enc, y_score)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            # Plot ROC curve\n",
        "            plt.plot(fpr, tpr, lw=2,\n",
        "                    label=f'{model_name} (AUC = {roc_auc:.3f})',\n",
        "                    color=model_colors[model_name])\n",
        "\n",
        "        # Plot diagonal line\n",
        "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "\n",
        "        # Set plot details\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate', fontsize=12)\n",
        "        plt.ylabel('True Positive Rate', fontsize=12)\n",
        "        plt.title('ROC Curve - Binary Classification', fontsize=16, pad=20)\n",
        "        plt.legend(loc=\"lower right\", fontsize=12)\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "    # Multi-class case\n",
        "    else:\n",
        "        # Create subplot grid based on number of classes\n",
        "        fig, axes = plt.subplots(nrows=(n_classes+1)//2, ncols=2, figsize=(16, 4*((n_classes+1)//2)))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # For each class\n",
        "        for i, cls in enumerate(classes):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Prepare binary labels (one-vs-rest)\n",
        "            y_test_bin = (y_test_enc == i).astype(int)\n",
        "\n",
        "            # Plot ROC for each model for this class\n",
        "            for model_name, model in models_dict.items():\n",
        "                # Get predictions for this class\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    y_score = model.predict_proba(X_test)[:, i]\n",
        "                else:\n",
        "                    # For models without predict_proba\n",
        "                    y_score = (model.predict(X_test) == i).astype(int)\n",
        "\n",
        "                # Compute ROC curve and area\n",
        "                fpr, tpr, _ = roc_curve(y_test_bin, y_score)\n",
        "                roc_auc = auc(fpr, tpr)\n",
        "\n",
        "                # Plot ROC curve\n",
        "                ax.plot(fpr, tpr, lw=2,\n",
        "                        label=f'{model_name} (AUC = {roc_auc:.3f})',\n",
        "                        color=model_colors[model_name])\n",
        "\n",
        "            # Plot diagonal line\n",
        "            ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "\n",
        "            # Set plot details\n",
        "            ax.set_xlim([0.0, 1.0])\n",
        "            ax.set_ylim([0.0, 1.05])\n",
        "            ax.set_xlabel('False Positive Rate', fontsize=10)\n",
        "            ax.set_ylabel('True Positive Rate', fontsize=10)\n",
        "            ax.set_title(f'Class: {cls}', fontsize=12)\n",
        "            ax.legend(loc=\"lower right\", fontsize=8)\n",
        "            ax.grid(alpha=0.3)\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for j in range(i+1, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.suptitle('ROC Curves - One-vs-Rest', fontsize=16, y=1.02)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Precision-Recall Curve plotting function\n",
        "def plot_precision_recall_curves(models_dict, X_test, y_test, classes, save_path):\n",
        "    \"\"\"Plot Precision-Recall curves for multiple models\"\"\"\n",
        "    n_classes = len(classes)\n",
        "    le = LabelEncoder().fit(classes)\n",
        "    y_test_enc = le.transform(y_test)\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # If binary classification\n",
        "    if n_classes == 2:\n",
        "        for model_name, model in models_dict.items():\n",
        "            # Get predictions\n",
        "            y_score = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "            # Compute precision-recall curve and average precision\n",
        "            precision, recall, _ = precision_recall_curve(y_test_enc, y_score)\n",
        "            avg_precision = average_precision_score(y_test_enc, y_score)\n",
        "\n",
        "            # Plot precision-recall curve\n",
        "            plt.plot(recall, precision, lw=2,\n",
        "                    label=f'{model_name} (AP = {avg_precision:.3f})',\n",
        "                    color=model_colors[model_name])\n",
        "\n",
        "        # Set plot details\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('Recall', fontsize=12)\n",
        "        plt.ylabel('Precision', fontsize=12)\n",
        "        plt.title('Precision-Recall Curve - Binary Classification', fontsize=16, pad=20)\n",
        "        plt.legend(loc=\"best\", fontsize=12)\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "    # Multi-class case\n",
        "    else:\n",
        "        # Create subplot grid based on number of classes\n",
        "        fig, axes = plt.subplots(nrows=(n_classes+1)//2, ncols=2, figsize=(16, 4*((n_classes+1)//2)))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # For each class\n",
        "        for i, cls in enumerate(classes):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Prepare binary labels (one-vs-rest)\n",
        "            y_test_bin = (y_test_enc == i).astype(int)\n",
        "\n",
        "            # Plot PR curve for each model for this class\n",
        "            for model_name, model in models_dict.items():\n",
        "                # Get predictions for this class\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    y_score = model.predict_proba(X_test)[:, i]\n",
        "                else:\n",
        "                    # For models without predict_proba\n",
        "                    y_score = (model.predict(X_test) == i).astype(int)\n",
        "\n",
        "                # Compute precision-recall curve and average precision\n",
        "                precision, recall, _ = precision_recall_curve(y_test_bin, y_score)\n",
        "                avg_precision = average_precision_score(y_test_bin, y_score)\n",
        "\n",
        "                # Plot precision-recall curve\n",
        "                ax.plot(recall, precision, lw=2,\n",
        "                        label=f'{model_name} (AP = {avg_precision:.3f})',\n",
        "                        color=model_colors[model_name])\n",
        "\n",
        "            # Set plot details\n",
        "            ax.set_xlim([0.0, 1.0])\n",
        "            ax.set_ylim([0.0, 1.05])\n",
        "            ax.set_xlabel('Recall', fontsize=10)\n",
        "            ax.set_ylabel('Precision', fontsize=10)\n",
        "            ax.set_title(f'Class: {cls}', fontsize=12)\n",
        "            ax.legend(loc=\"best\", fontsize=8)\n",
        "            ax.grid(alpha=0.3)\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for j in range(i+1, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.suptitle('Precision-Recall Curves - One-vs-Rest', fontsize=16, y=1.02)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Performance metrics visualization\n",
        "def plot_model_comparison(results, save_path):\n",
        "    \"\"\"Plot model performance comparison with robust metric handling\"\"\"\n",
        "    # Extract available metrics from results - check what's actually available\n",
        "    # Different sklearn versions might have different keys in classification_report\n",
        "    model_names = list(results.keys())\n",
        "\n",
        "    # Check which metrics are actually available\n",
        "    available_metrics = set()\n",
        "    for model in model_names:\n",
        "        for metric in results[model]['report']['weighted avg'].keys():\n",
        "            if metric not in ['support']:  # Exclude non-score metrics\n",
        "                available_metrics.add(metric)\n",
        "\n",
        "    # Define the metrics to use - using what's available\n",
        "    metrics = ['precision', 'recall', 'f1-score']  # These should always be available\n",
        "\n",
        "    # Create data for plotting\n",
        "    comparison_data = []\n",
        "    for metric in metrics:\n",
        "        for model in model_names:\n",
        "            if metric in results[model]['report']['weighted avg']:\n",
        "                comparison_data.append({\n",
        "                    'Model': model,\n",
        "                    'Metric': metric,\n",
        "                    'Value': results[model]['report']['weighted avg'][metric]\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Warning: Metric '{metric}' not found for model '{model}'. Using 0.\")\n",
        "                comparison_data.append({\n",
        "                    'Model': model,\n",
        "                    'Metric': metric,\n",
        "                    'Value': 0.0\n",
        "                })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_metrics = pd.DataFrame(comparison_data)\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # Create grouped bar chart\n",
        "    chart = sns.barplot(x='Model', y='Value', hue='Metric', data=df_metrics, palette='viridis')\n",
        "\n",
        "    # Add value labels\n",
        "    for i, container in enumerate(chart.containers):\n",
        "        for j, bar in enumerate(container):\n",
        "            if bar.get_height() is not None:  # Ensure the bar has a height\n",
        "                height = bar.get_height()\n",
        "                chart.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                          f'{height:.3f}', ha='center', va='bottom',\n",
        "                          fontsize=8, rotation=0)\n",
        "\n",
        "    # Set plot details\n",
        "    plt.title('Model Performance Comparison (Weighted Metrics)', fontsize=16, pad=20)\n",
        "    plt.xlabel('Model', fontsize=12)\n",
        "    plt.ylabel('Score', fontsize=12)\n",
        "    plt.ylim(0, 1.05)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.legend(title='Metric', fontsize=10, title_fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Feature importance visualization\n",
        "def plot_feature_importance_model(model, feature_names, save_path, max_features=20, model_name=\"Model\"):\n",
        "    \"\"\"Plot feature importances from model\"\"\"\n",
        "    # Get feature importances\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        importances = model.feature_importances_\n",
        "    elif hasattr(model, 'coef_'):\n",
        "        importances = np.abs(model.coef_).mean(axis=0) if model.coef_.ndim > 1 else np.abs(model.coef_)\n",
        "    else:\n",
        "        print(f\"Model {model_name} does not have feature_importances_ or coef_ attribute\")\n",
        "        return\n",
        "\n",
        "    # Sort and get top features\n",
        "    indices = np.argsort(importances)[::-1][:max_features]\n",
        "    top_importances = importances[indices]\n",
        "    top_features = [feature_names[i] for i in indices]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.barh(range(len(top_features)), top_importances, align='center', color=colors)\n",
        "    plt.yticks(range(len(top_features)), top_features)\n",
        "    plt.title(f'Feature Importance - {model_name}', fontsize=16, pad=20)\n",
        "    plt.xlabel('Importance', fontsize=12)\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# Main training loop\n",
        "results = {}\n",
        "le = LabelEncoder().fit(y_train_res)\n",
        "y_train_encoded = le.transform(y_train_res)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "for name in ['LightGBM', 'XGBoost', 'RandomForest']:  # Ordered by memory efficiency\n",
        "    print(f\"\\nTraining {name} - Memory Usage: {check_memory()}%\")\n",
        "\n",
        "    if check_memory() > 85:\n",
        "        print(\"Memory threshold exceeded - skipping remaining models\")\n",
        "        break\n",
        "\n",
        "    # Reduced search space\n",
        "    search = RandomizedSearchCV(\n",
        "        models[name],\n",
        "        param_grids[name],\n",
        "        n_iter=2,  # Reduced from 5\n",
        "        cv=2,      # Reduced from 3\n",
        "        scoring='f1_weighted',\n",
        "        random_state=42,\n",
        "        n_jobs=1   # Reduced parallelism\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Plot learning curve before full training\n",
        "        plot_learning_curve(\n",
        "            models[name], X_train_scaled, y_train_encoded,\n",
        "            train_sizes=np.linspace(0.1, 1.0, 5),\n",
        "            cv=2,\n",
        "            save_path=os.path.join(viz_dir, f'learning_curve_{name}.png'),\n",
        "            title=f\"Learning Curve - {name}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not create learning curve for {name}: {str(e)}\")\n",
        "\n",
        "    search.fit(X_train_scaled, y_train_encoded)\n",
        "\n",
        "    # Capture parameters before cleanup\n",
        "    best_params = search.best_params_\n",
        "    best_model = search.best_estimator_\n",
        "\n",
        "    # Save model immediately\n",
        "    model_path = os.path.join(results_dir, f'best_{name}.pkl')\n",
        "    joblib.dump(best_model, model_path)\n",
        "    print(f\"Saved {name} model\")\n",
        "\n",
        "    # Clean up search object\n",
        "    del search\n",
        "    gc.collect()\n",
        "\n",
        "    # Evaluate and store results\n",
        "    y_pred = best_model.predict(X_test_scaled)\n",
        "    y_pred_proba = best_model.predict_proba(X_test_scaled) if hasattr(best_model, 'predict_proba') else None\n",
        "\n",
        "    # Calculate metrics\n",
        "    report_dict = classification_report(y_test_encoded, y_pred, output_dict=True)\n",
        "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "    f1 = f1_score(y_test_encoded, y_pred, average='weighted')\n",
        "    precision = precision_score(y_test_encoded, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test_encoded, y_pred, average='weighted')\n",
        "\n",
        "    try:\n",
        "        # Create confusion matrix\n",
        "        plot_confusion_matrix(\n",
        "            y_test, y_pred,\n",
        "            classes=np.unique(y_test),\n",
        "            save_path=os.path.join(viz_dir, f'confusion_matrix_{name}.png'),\n",
        "            title=f\"Confusion Matrix - {name}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not create confusion matrix for {name}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        # Plot feature importance if available\n",
        "        if hasattr(best_model, 'feature_importances_') or hasattr(best_model, 'coef_'):\n",
        "            plot_feature_importance_model(\n",
        "                best_model, X.columns,\n",
        "                save_path=os.path.join(viz_dir, f'feature_importance_{name}.png'),\n",
        "                model_name=name\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not create feature importance plot for {name}: {str(e)}\")\n",
        "\n",
        "    # Add explicit report keys for all models\n",
        "    # This ensures we always have the same metrics available\n",
        "    results[name] = {\n",
        "        'model': best_model,\n",
        "        'report': report_dict,\n",
        "        'params': best_params,\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "    # Clean up temporary variables\n",
        "    del y_pred, best_model\n",
        "    gc.collect()\n",
        "\n",
        "# 10) Model Performance Comparison and Visualization\n",
        "if results:\n",
        "    try:\n",
        "        # Collect all trained models for ROC and PR curve plotting\n",
        "        trained_models = {name: results[name]['model'] for name in results}\n",
        "\n",
        "        # Plot ROC curves for all models\n",
        "        plot_roc_curves(\n",
        "            trained_models, X_test_scaled, y_test,\n",
        "            classes=np.unique(y_test),\n",
        "            save_path=os.path.join(viz_dir, 'roc_curves.png')\n",
        "        )\n",
        "\n",
        "        # Plot Precision-Recall curves for all models\n",
        "        plot_precision_recall_curves(\n",
        "            trained_models, X_test_scaled, y_test,\n",
        "            classes=np.unique(y_test),\n",
        "            save_path=os.path.join(viz_dir, 'precision_recall_curves.png')\n",
        "        )\n",
        "\n",
        "        # Plot model performance comparison\n",
        "        plot_model_comparison(results, os.path.join(viz_dir, 'model_comparison.png'))\n",
        "\n",
        "        # Create learning rate comparison for LightGBM and XGBoost\n",
        "        if 'LightGBM' in results and 'XGBoost' in results:\n",
        "            try:\n",
        "                # Check if evaluation results are available in the models\n",
        "                has_lgbm_history = hasattr(results['LightGBM']['model'], 'evals_result_') and results['LightGBM']['model'].evals_result_\n",
        "                has_xgb_history = hasattr(results['XGBoost']['model'], 'evals_result_') and results['XGBoost']['model'].evals_result_\n",
        "\n",
        "                if has_lgbm_history or has_xgb_history:\n",
        "                    # Plot learning rate\n",
        "                    plt.figure(figsize=(12, 6))\n",
        "\n",
        "                    # Check and plot LightGBM history if available\n",
        "                    if has_lgbm_history:\n",
        "                        evals_lgbm = results['LightGBM']['model'].evals_result_\n",
        "                        # Check which keys are actually available in the evals_result_ dictionary\n",
        "                        lgbm_keys = list(evals_lgbm.keys())\n",
        "                        if lgbm_keys:  # If there are keys available\n",
        "                            train_key = lgbm_keys[0]  # First key is typically training data\n",
        "                            valid_key = lgbm_keys[1] if len(lgbm_keys) > 1 else None  # Second key is validation data if available\n",
        "\n",
        "                            # Get metric names\n",
        "                            metric_keys = list(evals_lgbm[train_key].keys())\n",
        "                            if metric_keys:  # If there are metric keys available\n",
        "                                metric_key = metric_keys[0]  # Use the first metric\n",
        "\n",
        "                                # Plot training data\n",
        "                                plt.plot(evals_lgbm[train_key][metric_key],\n",
        "                                        label=f'LightGBM Train ({metric_key})', color=colors[0])\n",
        "\n",
        "                                # Plot validation data if available\n",
        "                                if valid_key:\n",
        "                                    plt.plot(evals_lgbm[valid_key][metric_key],\n",
        "                                            label=f'LightGBM Validation ({metric_key})', color=colors[1])\n",
        "\n",
        "                    # Check and plot XGBoost history if available\n",
        "                    if has_xgb_history:\n",
        "                        evals_xgb = results['XGBoost']['model'].evals_result_\n",
        "                        # Check which keys are actually available\n",
        "                        xgb_keys = list(evals_xgb.keys())\n",
        "                        if xgb_keys:  # If there are keys available\n",
        "                            train_key = next((k for k in xgb_keys if 'train' in k.lower()), xgb_keys[0])\n",
        "                            valid_key = next((k for k in xgb_keys if 'val' in k.lower() or 'test' in k.lower()),\n",
        "                                           xgb_keys[1] if len(xgb_keys) > 1 else None)\n",
        "\n",
        "                            # Get metric names\n",
        "                            train_metrics = list(evals_xgb[train_key].keys()) if train_key else []\n",
        "                            if train_metrics:  # If there are metric keys available\n",
        "                                metric_key = train_metrics[0]  # Use the first metric\n",
        "\n",
        "                                # Plot training data\n",
        "                                plt.plot(evals_xgb[train_key][metric_key],\n",
        "                                        label=f'XGBoost Train ({metric_key})', color=colors[2])\n",
        "\n",
        "                                # Plot validation data if available\n",
        "                                if valid_key and metric_key in evals_xgb[valid_key]:\n",
        "                                    plt.plot(evals_xgb[valid_key][metric_key],\n",
        "                                            label=f'XGBoost Validation ({metric_key})', color=colors[3])\n",
        "\n",
        "                    plt.title('Learning Curves Comparison', fontsize=16, pad=20)\n",
        "                    plt.xlabel('Boosting Round', fontsize=12)\n",
        "                    plt.ylabel('Loss', fontsize=12)\n",
        "                    plt.legend(fontsize=12)\n",
        "                    plt.grid(alpha=0.3)\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(viz_dir, 'learning_curves_comparison.png'), bbox_inches='tight')\n",
        "                    plt.close()\n",
        "                else:\n",
        "                    print(\"No evaluation results available for learning curve comparison\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Could not create learning curves comparison: {str(e)}\")\n",
        "                print(\"Continuing with other visualizations...\")\n",
        "\n",
        "        # Save best model\n",
        "        best_model_name = max(results, key=lambda x: results[x]['report']['weighted avg']['f1-score'])\n",
        "        final_model = results[best_model_name]['model']\n",
        "        joblib.dump(final_model, os.path.join(results_dir, 'final_model.pkl'))\n",
        "\n",
        "        # Create summary of best model\n",
        "        with open(os.path.join(results_dir, 'best_model_summary.txt'), 'w') as f:\n",
        "            f.write(f\"Best Model: {best_model_name}\\n\")\n",
        "            f.write(f\"Parameters: {results[best_model_name]['params']}\\n\\n\")\n",
        "            f.write(\"Performance Metrics:\\n\")\n",
        "            f.write(f\"Accuracy: {results[best_model_name]['accuracy']:.4f}\\n\")\n",
        "            f.write(f\"F1 Score (weighted): {results[best_model_name]['f1']:.4f}\\n\")\n",
        "            f.write(f\"Precision (weighted): {results[best_model_name]['precision']:.4f}\\n\")\n",
        "            f.write(f\"Recall (weighted): {results[best_model_name]['recall']:.4f}\\n\\n\")\n",
        "            f.write(\"Classification Report:\\n\")\n",
        "\n",
        "            # Format the classification report\n",
        "            report = results[best_model_name]['report']\n",
        "            for cls in report:\n",
        "                if cls not in ['macro avg', 'weighted avg']:\n",
        "                    f.write(f\"Class {cls}:\\n\")\n",
        "                    for metric in ['precision', 'recall', 'f1-score', 'support']:\n",
        "                        if metric in report[cls]:\n",
        "                            f.write(f\"  {metric}: {report[cls][metric]:.4f}\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "\n",
        "            for avg in ['macro avg', 'weighted avg']:\n",
        "                if avg in report:\n",
        "                    f.write(f\"{avg}:\\n\")\n",
        "                    for metric in ['precision', 'recall', 'f1-score']:\n",
        "                        if metric in report[avg]:\n",
        "                            f.write(f\"  {metric}: {report[avg][metric]:.4f}\\n\")\n",
        "                    f.write(\"\\n\")\n",
        "\n",
        "        # Generate simplified report\n",
        "        final_report = pd.DataFrame({\n",
        "            model: results[model]['report']['weighted avg']\n",
        "            for model in results\n",
        "        }).T\n",
        "        final_report.to_csv(os.path.join(results_dir, 'performance_report.csv'))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in visualization section: {str(e)}\")\n",
        "        print(\"Continuing with remaining visualizations...\")\n",
        "\n",
        "    # 11) Summary Dashboard for Paper\n",
        "    # Create a combined visualization dashboard for paper\n",
        "    try:\n",
        "        plt.figure(figsize=(20, 24))\n",
        "        gs = gridspec.GridSpec(4, 2, height_ratios=[1, 1, 1, 1])\n",
        "\n",
        "        # Function to safely load and display an image\n",
        "        def safe_load_image(ax, image_path, title):\n",
        "            try:\n",
        "                if os.path.exists(image_path):\n",
        "                    img = plt.imread(image_path)\n",
        "                    ax.imshow(img)\n",
        "                    ax.axis('off')\n",
        "                    ax.set_title(title, fontsize=14, loc='left')\n",
        "                else:\n",
        "                    ax.text(0.5, 0.5, f\"Image not found:\\n{os.path.basename(image_path)}\",\n",
        "                           ha='center', va='center', fontsize=12, color='red')\n",
        "                    ax.axis('off')\n",
        "            except Exception as e:\n",
        "                ax.text(0.5, 0.5, f\"Error loading image:\\n{str(e)}\",\n",
        "                       ha='center', va='center', fontsize=10, color='red')\n",
        "                ax.axis('off')\n",
        "\n",
        "        # Class Distribution\n",
        "        ax1 = plt.subplot(gs[0, 0])\n",
        "        safe_load_image(ax1, os.path.join(viz_dir, 'class_distribution.png'), 'A) Class Distribution')\n",
        "\n",
        "        # SMOTE Effect\n",
        "        ax2 = plt.subplot(gs[0, 1])\n",
        "        safe_load_image(ax2, os.path.join(viz_dir, 'smote_effect.png'), 'B) Effect of SMOTE')\n",
        "\n",
        "        # PCA Visualization\n",
        "        ax3 = plt.subplot(gs[1, 0])\n",
        "        safe_load_image(ax3, os.path.join(viz_dir, 'pca_visualization.png'), 'C) PCA Visualization')\n",
        "\n",
        "        # t-SNE Visualization\n",
        "        ax4 = plt.subplot(gs[1, 1])\n",
        "        safe_load_image(ax4, os.path.join(viz_dir, 'tsne_training_data.png'), 'D) t-SNE Visualization')\n",
        "\n",
        "        # ROC Curves\n",
        "        ax5 = plt.subplot(gs[2, 0])\n",
        "        safe_load_image(ax5, os.path.join(viz_dir, 'roc_curves.png'), 'E) ROC Curves')\n",
        "\n",
        "        # PR Curves\n",
        "        ax6 = plt.subplot(gs[2, 1])\n",
        "        safe_load_image(ax6, os.path.join(viz_dir, 'precision_recall_curves.png'), 'F) Precision-Recall Curves')\n",
        "\n",
        "        # Confusion Matrix of Best Model\n",
        "        ax7 = plt.subplot(gs[3, 0])\n",
        "        cm_path = os.path.join(viz_dir, f'confusion_matrix_{best_model_name}.png')\n",
        "        # If best model confusion matrix doesn't exist, try using any existing one\n",
        "        if not os.path.exists(cm_path):\n",
        "            for model_name in results:\n",
        "                alt_path = os.path.join(viz_dir, f'confusion_matrix_{model_name}.png')\n",
        "                if os.path.exists(alt_path):\n",
        "                    cm_path = alt_path\n",
        "                    print(f\"Using confusion matrix from {model_name} instead of best model\")\n",
        "                    break\n",
        "\n",
        "        safe_load_image(ax7, cm_path, f'G) Confusion Matrix')\n",
        "\n",
        "        # Model Comparison\n",
        "        ax8 = plt.subplot(gs[3, 1])\n",
        "        safe_load_image(ax8, os.path.join(viz_dir, 'model_comparison.png'), 'H) Model Performance Comparison')\n",
        "\n",
        "        plt.suptitle('Comprehensive Analysis of Syscalls Classification', fontsize=20, y=0.995)\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
        "        plt.savefig(os.path.join(results_dir, 'paper_visualization_dashboard.png'),\n",
        "                   dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Successfully created summary dashboard at {os.path.join(results_dir, 'paper_visualization_dashboard.png')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating summary dashboard: {str(e)}\")\n",
        "        print(\"Continuing with the rest of the analysis...\")\n",
        "\n",
        "# 12) Create summary PDF with all visualizations for the paper\n",
        "try:\n",
        "    from fpdf import FPDF\n",
        "\n",
        "    class PDF(FPDF):\n",
        "        def header(self):\n",
        "            # Logo\n",
        "            # self.image('logo.png', 10, 8, 33)\n",
        "            # Arial bold 15\n",
        "            self.set_font('Arial', 'B', 15)\n",
        "            # Move to the right\n",
        "            self.cell(80)\n",
        "            # Title\n",
        "            self.cell(30, 10, 'Machine Learning Analysis for Syscalls Classification', 0, 0, 'C')\n",
        "            # Line break\n",
        "            self.ln(20)\n",
        "\n",
        "        def footer(self):\n",
        "            # Position at 1.5 cm from bottom\n",
        "            self.set_y(-15)\n",
        "            # Arial italic 8\n",
        "            self.set_font('Arial', 'I', 8)\n",
        "            # Page number\n",
        "            self.cell(0, 10, 'Page ' + str(self.page_no()) + '/{nb}', 0, 0, 'C')\n",
        "\n",
        "    # Create PDF\n",
        "    pdf = PDF()\n",
        "    pdf.alias_nb_pages()\n",
        "    pdf.add_page()\n",
        "    pdf.set_font('Arial', 'B', 12)\n",
        "\n",
        "    # Title\n",
        "    pdf.cell(0, 10, 'Comprehensive Visualization Report', 0, 1, 'C')\n",
        "    pdf.ln(5)\n",
        "\n",
        "    # Add all visualizations with descriptions\n",
        "    visualizations = [\n",
        "        ('class_distribution.png', 'Class Distribution'),\n",
        "        ('smote_effect.png', 'Effect of SMOTE Oversampling'),\n",
        "        ('feature_correlation.png', 'Feature Correlation Matrix'),\n",
        "        ('feature_importance.png', 'Feature Importance Analysis'),\n",
        "        ('pca_explained_variance.png', 'PCA Explained Variance'),\n",
        "        ('pca_visualization.png', 'PCA Visualization'),\n",
        "        ('tsne_training_data.png', 't-SNE Visualization'),\n",
        "        ('roc_curves.png', 'ROC Curves'),\n",
        "        ('precision_recall_curves.png', 'Precision-Recall Curves')\n",
        "    ]\n",
        "\n",
        "    # Add model-specific visualizations\n",
        "    for model_name in results:\n",
        "        visualizations.append((f'confusion_matrix_{model_name}.png', f'Confusion Matrix - {model_name}'))\n",
        "        visualizations.append((f'learning_curve_{model_name}.png', f'Learning Curve - {model_name}'))\n",
        "        if os.path.exists(os.path.join(viz_dir, f'feature_importance_{model_name}.png')):\n",
        "            visualizations.append((f'feature_importance_{model_name}.png', f'Feature Importance - {model_name}'))\n",
        "\n",
        "    # Add comparison visualizations\n",
        "    visualizations.append(('model_comparison.png', 'Model Performance Comparison'))\n",
        "    if os.path.exists(os.path.join(viz_dir, 'learning_curves_comparison.png')):\n",
        "        visualizations.append(('learning_curves_comparison.png', 'Learning Curves Comparison'))\n",
        "\n",
        "    # Add dashboard\n",
        "    visualizations.append(('paper_visualization_dashboard.png', 'Comprehensive Dashboard'))\n",
        "\n",
        "    # Add visualizations to PDF\n",
        "    for i, (viz_file, viz_desc) in enumerate(visualizations):\n",
        "        if os.path.exists(os.path.join(viz_dir, viz_file)):\n",
        "            if i > 0 and i % 2 == 0:\n",
        "                pdf.add_page()\n",
        "\n",
        "            pdf.set_font('Arial', 'B', 11)\n",
        "            pdf.cell(0, 10, viz_desc, 0, 1)\n",
        "            pdf.image(os.path.join(viz_dir, viz_file), x=10, w=190)\n",
        "            pdf.ln(5)\n",
        "\n",
        "    # Add performance summary\n",
        "    pdf.add_page()\n",
        "    pdf.set_font('Arial', 'B', 14)\n",
        "    pdf.cell(0, 10, 'Model Performance Summary', 0, 1, 'C')\n",
        "    pdf.ln(5)\n",
        "\n",
        "    # Add summary table\n",
        "    pdf.set_font('Arial', '', 10)\n",
        "    col_width = 40\n",
        "    row_height = 10\n",
        "\n",
        "    # Table header\n",
        "    pdf.set_font('Arial', 'B', 10)\n",
        "    pdf.cell(col_width, row_height, 'Model', 1, 0, 'C')\n",
        "    pdf.cell(col_width, row_height, 'Accuracy', 1, 0, 'C')\n",
        "    pdf.cell(col_width, row_height, 'F1 Score', 1, 0, 'C')\n",
        "    pdf.cell(col_width, row_height, 'Precision', 1, 1, 'C')\n",
        "\n",
        "    # Table rows\n",
        "    pdf.set_font('Arial', '', 10)\n",
        "    for model_name in results:\n",
        "        pdf.cell(col_width, row_height, model_name, 1, 0, 'L')\n",
        "        pdf.cell(col_width, row_height, f\"{results[model_name]['accuracy']:.4f}\", 1, 0, 'C')\n",
        "        pdf.cell(col_width, row_height, f\"{results[model_name]['f1']:.4f}\", 1, 0, 'C')\n",
        "        pdf.cell(col_width, row_height, f\"{results[model_name]['precision']:.4f}\", 1, 1, 'C')\n",
        "\n",
        "    # Best model highlight\n",
        "    pdf.ln(5)\n",
        "    pdf.set_font('Arial', 'B', 11)\n",
        "    pdf.cell(0, 10, f\"Best Model: {best_model_name}\", 0, 1)\n",
        "    pdf.set_font('Arial', '', 10)\n",
        "    pdf.multi_cell(0, 10, f\"Parameters: {results[best_model_name]['params']}\")\n",
        "\n",
        "    # Save PDF\n",
        "    pdf.output(os.path.join(results_dir, 'visualization_report.pdf'))\n",
        "    print(f\"Generated visualization report PDF at {os.path.join(results_dir, 'visualization_report.pdf')}\")\n",
        "except ImportError:\n",
        "    print(\"FPDF not installed. Skipping PDF generation.\")\n",
        "    print(\"To generate PDF, install fpdf: !pip install fpdf\")\n",
        "\n",
        "print(\"Pipeline completed successfully. Memory usage:\", check_memory())\n",
        "print(f\"All visualizations saved to {viz_dir}\")\n",
        "print(f\"Summary dashboard saved to {os.path.join(results_dir, 'paper_visualization_dashboard.png')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "Uk_njQvOm7u9",
        "outputId": "65ad2f41-3b68-4471-c7c9-249e84956cc6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Initial memory usage: 6.99 MB\n",
            "Train counts: {1: np.int64(1128), 2: np.int64(1890), 3: np.int64(3514), 4: np.int64(2291), 5: np.int64(1615)}\n",
            "Test counts: {1: np.int64(125), 2: np.int64(210), 3: np.int64(390), 4: np.int64(255), 5: np.int64(180)}\n",
            "Train percentages: {1: np.float64(10.806667944050584), 2: np.float64(18.106917033914545), 3: np.float64(33.66545315194482), 4: np.float64(21.948649166506996), 5: np.float64(15.472312703583063)}\n",
            "Test percentages: {1: np.float64(10.775862068965516), 2: np.float64(18.103448275862068), 3: np.float64(33.62068965517241), 4: np.float64(21.982758620689655), 5: np.float64(15.517241379310345)}\n",
            "Number of components needed for 95% variance: 20\n",
            "PCA transformation complete. Output shape: (17570, 20)\n",
            "\n",
            "Training LightGBM - Memory Usage: 37.1%\n",
            "Saved LightGBM model\n",
            "\n",
            "Training XGBoost - Memory Usage: 42.7%\n",
            "Saved XGBoost model\n",
            "\n",
            "Training RandomForest - Memory Usage: 44.1%\n",
            "Saved RandomForest model\n",
            "No evaluation results available for learning curve comparison\n",
            "Error in visualization section: argument of type 'float' is not iterable\n",
            "Continuing with remaining visualizations...\n",
            "Successfully created summary dashboard at /content/drive/MyDrive/Amir-Khan FYP/Cluade-Results/paper_visualization_dashboard.png\n",
            "Generated visualization report PDF at /content/drive/MyDrive/Amir-Khan FYP/Cluade-Results/visualization_report.pdf\n",
            "Pipeline completed successfully. Memory usage: 54.7\n",
            "All visualizations saved to /content/drive/MyDrive/Amir-Khan FYP/Cluade-Results/Visualizations\n",
            "Summary dashboard saved to /content/drive/MyDrive/Amir-Khan FYP/Cluade-Results/paper_visualization_dashboard.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 4800x3600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 4800x3600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
